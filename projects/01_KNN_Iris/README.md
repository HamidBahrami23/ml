# ğŸŒ¼ KNN Classification â€“ Iris Dataset

## ğŸ“Œ Overview
This project applies **K-Nearest Neighbors (KNN)** to classify flower types in the well-known **Iris dataset**.  
The goal is to understand how KNN works on a simple dataset and explore model evaluation and hyperparameter tuning.

---

## ğŸ“‚ Project Structure


01_KNN_Iris/
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ 01_KNN_Iris.ipynb # Main notebook
â”œâ”€â”€ src/ # Optional scripts (not used in this project)
â””â”€â”€ data/ # Dataset (ignored by .gitignore)

---

## ğŸ§  Key Concepts
- Train/test splitting
- Feature scaling using `StandardScaler`
- Distance-based learning (KNN)
- Model evaluation using:
  - Accuracy
  - Confusion Matrix
  - Classification Report (Precision/Recall/F1)
- Hyperparameter Tuning (finding best **K**)

---

## âœ… Result Summary
- Best value of **K (number of neighbors): `9`**
- Test accuracy achieved: **0.9556 (â‰ˆ 96%)**

Confusion Matrix:

| Setosa | Versicolor | Virginica |
|--------|------------|-----------|
| âœ… perfect | âœ… perfect | ğŸ”¸ some misclassifications |

---

## ğŸ”§ Tech Stack
| Library | Usage |
|---------|--------|
| `pandas` / `numpy` | Data manipulation |
| `scikit-learn` | KNN model, scaling, evaluation |
| `matplotlib` | Visualization |

---

## â–¶ï¸ How to Run
```bash
pip install -r requirements.txt


Open notebook:

jupyter notebook notebooks/01_KNN_Iris.ipynb

---

##ğŸ’¡ Learnings

Scaling is critical in KNN.

Higher K reduces noise but too large K â†’ model becomes too general.

Visualization of K vs accuracy reveals the â€œelbow pointâ€.

##ğŸš€ Future Improvements

Try KNN on different datasets

Compare with other algorithms (SVM / Decision Tree / Logistic Regression)

