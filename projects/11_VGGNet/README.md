# ğŸ–¼ï¸ Custom VGGNet â€“ Men vs Women Classification

## ğŸ“Œ Overview
This project implements a custom VGG-like convolutional neural network for binary classification of men vs women images.  
The model uses a VGG-inspired architecture with increasing convolutional layers and max pooling, trained on a subset of the Men-Women dataset from Kaggle.

---

## ğŸ“‚ Project Structure

```
11_VGGNet/
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ 11_VGGNet.ipynb  # Main notebook with model implementation
â””â”€â”€ src/                # Optional scripts (not used in this project)
```

---

## ğŸ§  Key Concepts
- Custom CNN architecture inspired by VGGNet
- Convolutional layers with increasing filters (64 â†’ 128 â†’ 256 â†’ 512)
- Max pooling for downsampling
- Data augmentation (horizontal flip, rotation, zoom)
- Binary classification with sigmoid activation
- Adam optimizer with low learning rate (0.0001)

---

## âœ… Result Summary
- **Architecture**: Custom VGG-like with 5 conv blocks
- **Input size**: 224x224 RGB images
- **Dataset**: 2000 train, 400 validation, 400 test images
- **Training epochs**: 40
- **Final performance**: Model struggled with convergence, accuracy around 50% (random guessing level)

Confusion Matrix:

| Class | Performance |
|-------|-------------|
| Men   | Poor classification |
| Women | Poor classification |

---

## ğŸ”§ Tech Stack
| Library | Usage |
|---------|--------|
| `tensorflow` / `keras` | CNN model, training |
| `matplotlib` | Visualization |
| `numpy` | Data manipulation |

---

## â–¶ï¸ How to Run

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
2. Launch the notebook:
   ```bash
   jupyter notebook notebook/11_VGGNet.ipynb
   ```

---

## ğŸ’¡ Learnings

- Custom CNN architectures require careful hyperparameter tuning
- Learning rate and model complexity are critical for convergence
- Data augmentation helps but isn't sufficient for poor model design
- Transfer learning approaches often perform better than training from scratch

## ğŸš€ Future Improvements

- Implement transfer learning with pre-trained models (VGG16, ResNet)
- Use larger datasets or better data preprocessing
- Experiment with different architectures (ResNet, EfficientNet)
- Add regularization techniques (dropout, batch normalization)
- Fine-tune hyperparameters systematically